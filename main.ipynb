{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Not enough GPU hardware devices available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0fbc65f0ade6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# limit GPU growth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Not enough GPU hardware devices available'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphysical_device\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphysical_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Not enough GPU hardware devices available"
     ]
    }
   ],
   "source": [
    "# limit GPU growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, 'Not enough GPU hardware devices available'\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from blocks import *\n",
    "from losses import *\n",
    "from containers import *\n",
    "from utils import *\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note (height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config('./configs/funit_animals.yaml')\n",
    "networks = FUNIT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_train.txt\n",
      "\tNumber of classes: 119\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f37634879d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f37634879d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_train.txt\n",
      "\tNumber of classes: 119\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f3763579488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f3763579488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_test.txt\n",
      "\tNumber of classes: 30\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f3762902840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f3762902840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_test.txt\n",
      "\tNumber of classes: 30\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f3761f51378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f3761f51378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n"
     ]
    }
   ],
   "source": [
    "datasets = get_datasets(config)\n",
    "train_content_dataset = datasets[0]\n",
    "train_class_dataset = datasets[1]\n",
    "train_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = config['max_iter']\n",
    "for epoch in range(epochs):\n",
    "    for (co_data, cl_data) in train_dataset:\n",
    "        train_returns = train_step(networks,co_data,cl_data,config)\n",
    "        print(\" G_loss: %.4f, D_loss: %.4f\" % (train_returns['G_loss'],train_returns['D_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--config',\n",
    "                        type=str,\n",
    "                        default='../FUNIT/configs/funit_animals.yaml',\n",
    "                        help='configuration file for training and testing')\n",
    "    parser.add_argument('--batch_size', type=int, default=0)\n",
    "    parser.add_argument('--output_path',\n",
    "                        type=str,\n",
    "                        default='.',\n",
    "                        help=\"outputs path\")\n",
    "    parser.add_argument('--test_batch_size',\n",
    "                         type=int,\n",
    "                         default=4)\n",
    "    opts = parser.parse_args()\n",
    "    config = get_config(opts.config)\n",
    "    \n",
    "    epochs = config['max_iter']\n",
    "    if opts.batch_size != 0:\n",
    "        config['batch_size'] = opts.batch_size\n",
    "        \n",
    "    # Networks\n",
    "    networks = FUNIT(config)\n",
    "\n",
    "    # Datasets\n",
    "    datasets = get_datasets(config)\n",
    "    train_content_dataset = datasets[0]\n",
    "    train_class_dataset = datasets[1]\n",
    "    train_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))\n",
    "    test_content_dataset = datasets[2]\n",
    "    test_class_dataset = datasets[3]\n",
    "    test_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))\n",
    "    \n",
    "    # Mean loss\n",
    "    # lossnames = [\"G_loss\",\"D_loss\"]\n",
    "    # metrics_list = []\n",
    "    # for itemname in lossnames:\n",
    "    #     metrics_list.append(tf.keras.metrics.Mean(itemname, dtype=tf.float32))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for (co_data, cl_data) in train_dataset:\n",
    "            train_returns = train_step(co_data,cl_data,config)\n",
    "            print(\" G_loss: %.4d, D_loss: %.4d\" % (train_returns['G_loss'],train_returns['D_loss']))\n",
    "            # for idx, itemname in enumerate(lossnames):\n",
    "            #     metrics_list[idx](train_returns[itemname])\n",
    "                \n",
    "        # for idx, itemname in enumerate(lossnames):\n",
    "        #    print(\"    {}: {:.4f}\".format(itemname,metrics_list[idx].result()))\n",
    "        # for metric in metrics_list:\n",
    "        #     metric.reset_states()\n",
    "            \n",
    "        '''if epoch % config['image_save_iter'] == 0 or\\\n",
    "           epoch % config['image_display_iter'] == 0:\n",
    "            if epoch % config['image_save_iter'] == 0:\n",
    "                key_str = '%08d' % (epoch + 1)\n",
    "            else:\n",
    "                key_str = 'current'\n",
    "            output_train_dataset = train_dataset.take(opts.test_batch_size)\n",
    "            output_test_dataset = test_dataset.take(opts.test_batch_size)\n",
    "            for idx, (co_data, cl_data) in output_train_dataset.enumerate():\n",
    "                test_returns = test_step(co_data,cl_data,config)\n",
    "                write_images(zip(test_returns['xa'],test_returns['xr'],test_returns['xt'],test_returns['xb']), \n",
    "                             test_returns['display_list'],\n",
    "                             'train_%s_%02d' % (key_str, idx)\n",
    "                             max(config['crop_image_height'], config['crop_image_width']))\n",
    "            for idx, (co_data, cl_data) in output_test_dataset.enumerate():\n",
    "                test_returns = test_step(co_data,cl_data,config)\n",
    "                write_images(zip(test_returns['xa'],test_returns['xr'],test_returns['xt'],test_returns['xb']), \n",
    "                             test_returns['display_list'],\n",
    "                             'test_%s_%02d' % (key_str, idx)\n",
    "                             max(config['crop_image_height'], config['crop_image_width']))\n",
    "        '''     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(nets,co_data,cl_data,config):\n",
    "    xa, la = co_data\n",
    "    xb, lb = cl_data\n",
    "    return_items = {}\n",
    "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "        xt_g, xr, xa_gan_feat, xb_gan_feat = nets.gen_update(co_data,cl_data,config)\n",
    "        resp_real, real_gen_feat, xt_d, resp_fake, fake_gan_feat = nets.dis_update(co_data,cl_data,config)\n",
    "        \n",
    "        resp_xr_fake, xr_gan_feat = nets.dis(xr, la)\n",
    "        resp_xt_fake, xt_gan_feat = nets.dis(xt_g, lb)\n",
    "        # Generator - GAN loss\n",
    "        l_adv_t = GANloss.gen_loss(resp_xt_fake,lb)\n",
    "        l_adv_r = GANloss.gen_loss(resp_xr_fake,la)\n",
    "        l_adv = 0.5 * (l_adv_t + l_adv_r)\n",
    "        # Generator - Reconstruction loss\n",
    "        l_x_rec = recon_loss(xr, xa)\n",
    "        # Generator - Feature Matching loss\n",
    "        l_c_rec = featmatch_loss(xr_gan_feat, xa_gan_feat)\n",
    "        l_m_rec = featmatch_loss(xt_gan_feat, xb_gan_feat)\n",
    "        \n",
    "        G_loss = config['gan_w'] * l_adv +\\\n",
    "                 config['r_w'] * l_x_rec +\\\n",
    "                 config['fm_w'] * (l_c_rec + l_m_rec)\n",
    "        \n",
    "        # Discriminator - GAN loss\n",
    "        l_real = GANloss.dis_loss(resp_real, lb, 'real')\n",
    "        l_fake = GANloss.dis_loss(resp_fake, lb, 'fake')\n",
    "        # Discriminator - Gradient Penalty\n",
    "        l_reg = gradient_penalty(nets.dis, xb, lb)\n",
    "        \n",
    "        D_loss = config['gan_w'] * l_real +\\\n",
    "                 config['gan_w'] * l_fake +\\\n",
    "                 10 * l_reg\n",
    "        \n",
    "    # Update Gradient\n",
    "    # - Gradient computing\n",
    "    gen_grad = g_tape.gradient(G_loss, nets.gen.trainable_variables)\n",
    "    dis_grad = d_tape.gradient(D_loss, nets.dis.trainable_variables)\n",
    "    # - Optimizer\n",
    "    nets.opt_gen.apply_gradients(zip(gen_grad, nets.gen.trainable_variables))\n",
    "    nets.opt_dis.apply_gradients(zip(dis_grad, nets.dis.trainable_variables))\n",
    "    \n",
    "    return_items['G_loss'] = G_loss.numpy()\n",
    "    return_items['D_loss'] = D_loss.numpy()\n",
    "    return return_items\n",
    "\n",
    "def test_step(nets,co_data,cl_data,config):\n",
    "    xa, la = co_data\n",
    "    xb, lb = cl_data\n",
    "    return_items = {}\n",
    "    xt, xr, xa_gan_feat, xb_gan_feat = nets.gen_update(co_data,cl_data,config)\n",
    "    return_items['xa'] = xa.numpy()\n",
    "    return_items['xb'] = xb.numpy()\n",
    "    return_items['xr'] = xr.numpy()\n",
    "    return_items['xt'] = xt.numpy()\n",
    "    return_items['display_list'] = ['xa','xr','xt','xb']\n",
    "    return return_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of working processes of output.\n",
    "#================================ TensorBoard (later)\n",
    "#================================ Check Points (later)\n",
    "#================================ Distributed Training (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = tf.random.normal([64,128,128,3])\n",
    "test2 = tf.random.normal([2,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = ContentEncoder(downs=3,\n",
    "                    n_res=2,\n",
    "                    n_filters=64,\n",
    "                    norm='in',activation='relu',pad_type='reflect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_2 = layer(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.output_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networks.gen.E_content.output_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCon = networks.gen.E_content(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['gen']['nf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3779, shape=(4,), dtype=int32, numpy=array([ 64,  16,  16, 512], dtype=int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(test1_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block = ContentEncoder(3,2,64,'in','relu','reflect')\n",
    "# block = ClassEncoder(4,64,64,'none','relu','reflect')\n",
    "# block = Decoder(3,2,32,3,'relu','reflect')\n",
    "# block = MLP(32,256,3,'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = block(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
