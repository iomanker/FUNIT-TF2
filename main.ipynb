{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# limit GPU growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(len(physical_devices))\n",
    "assert len(physical_devices) > 0, 'Not enough GPU hardware devices available'\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from blocks import *\n",
    "from losses import *\n",
    "from containers import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from run_step import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note (height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config('./configs/funit_animals.yaml')\n",
    "networks = FUNIT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets(config)\n",
    "train_content_dataset = datasets[0]\n",
    "train_class_dataset = datasets[1]\n",
    "train_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = config['max_iter']\n",
    "for epoch in range(epochs):\n",
    "    for (co_data, cl_data) in train_dataset:\n",
    "        train_returns = train_step(networks,co_data,cl_data,config)\n",
    "        print(\" G_loss: %.4f, D_loss: %.4f\" % (train_returns['G_loss'],train_returns['D_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_train.txt\n",
      "\tNumber of classes: 119\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f5820d4ea60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f5820d4ea60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_train.txt\n",
      "\tNumber of classes: 119\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f56624d1c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f56624d1c80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_test.txt\n",
      "\tNumber of classes: 30\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f5661e6b8c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f5661e6b8c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_test.txt\n",
      "\tNumber of classes: 30\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f5662320510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7f5662320510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "epoch 0:\n",
      " G_loss: 3.2974, D_loss: 0.92487\r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--config',\n",
    "#                         type=str,\n",
    "#                         default='../FUNIT/configs/funit_animals.yaml',\n",
    "#                         help='configuration file for training and testing')\n",
    "#     parser.add_argument('--batch_size', type=int, default=0)\n",
    "#     parser.add_argument('--output_path',\n",
    "#                         type=str,\n",
    "#                         default='.',\n",
    "#                         help=\"outputs path\")\n",
    "#     parser.add_argument('--test_batch_size',\n",
    "#                          type=int,\n",
    "#                          default=4)\n",
    "#     opts = parser.parse_args()\n",
    "#     config = get_config(opts.config)\n",
    "    config = get_config('./configs/funit_animals.yaml')\n",
    "    \n",
    "    epochs = config['max_iter']\n",
    "#     if opts.batch_size != 0:\n",
    "#         config['batch_size'] = opts.batch_size\n",
    "        \n",
    "    # Networks\n",
    "    networks = FUNIT(config)\n",
    "\n",
    "    # Datasets\n",
    "    datasets = get_datasets(config)\n",
    "    train_content_dataset = datasets[0]\n",
    "    train_class_dataset = datasets[1]\n",
    "    train_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))\n",
    "    test_content_dataset = datasets[2]\n",
    "    test_class_dataset = datasets[3]\n",
    "    test_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))\n",
    "    \n",
    "    # Mean loss\n",
    "    lossnames = [\"G_loss\",\"D_loss\"]\n",
    "    metrics_list = []\n",
    "    for itemname in lossnames:\n",
    "        metrics_list.append(tf.keras.metrics.Mean(itemname, dtype=tf.float32))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch %d:\" % epoch)\n",
    "        for (co_data, cl_data) in train_dataset:\n",
    "            train_returns = train_step(networks,co_data,cl_data,config)\n",
    "            print(\" G_loss: %.4f, D_loss: %.4f\" % (train_returns['G_loss'],train_returns['D_loss']), end='\\r')\n",
    "            for idx, itemname in enumerate(lossnames):\n",
    "                metrics_list[idx](train_returns[itemname])\n",
    "                \n",
    "        for idx, itemname in enumerate(lossnames):\n",
    "            print(\"    {}: {:.4f}\".format(itemname,metrics_list[idx].result()))\n",
    "            metrics_list[idx].reset_states()\n",
    "            \n",
    "        if epoch % config['image_save_iter'] == 0 or\\\n",
    "           epoch % config['image_display_iter'] == 0:\n",
    "            if epoch % config['image_save_iter'] == 0:\n",
    "                key_str = '%08d' % (epoch + 1)\n",
    "            else:\n",
    "                key_str = 'current'\n",
    "            output_train_dataset = train_dataset.take(opts.test_batch_size)\n",
    "            output_test_dataset = test_dataset.take(opts.test_batch_size)\n",
    "            for idx, (co_data, cl_data) in output_train_dataset.enumerate():\n",
    "                test_returns = test_step(co_data,cl_data,config)\n",
    "                write_images(zip(test_returns['xa'],test_returns['xr'],test_returns['xt'],test_returns['xb']), \n",
    "                             test_returns['display_list'],\n",
    "                             'train_%s_%02d' % (key_str, idx),\n",
    "                             max(config['crop_image_height'], config['crop_image_width']))\n",
    "            for idx, (co_data, cl_data) in output_test_dataset.enumerate():\n",
    "                test_returns = test_step(co_data,cl_data,config)\n",
    "                write_images(zip(test_returns['xa'],test_returns['xr'],test_returns['xt'],test_returns['xb']), \n",
    "                             test_returns['display_list'],\n",
    "                             'test_%s_%02d' % (key_str, idx),\n",
    "                             max(config['crop_image_height'], config['crop_image_width']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of working processes of output.\n",
    "#================================ TensorBoard (later)\n",
    "#================================ Check Points (later)\n",
    "#================================ Distributed Training (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = tf.random.normal([64,128,128,3])\n",
    "test2 = tf.random.normal([2,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block = ContentEncoder(3,2,64,'in','relu','reflect')\n",
    "# block = ClassEncoder(4,64,64,'none','relu','reflect')\n",
    "# block = Decoder(3,2,32,3,'relu','reflect')\n",
    "# block = MLP(32,256,3,'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
