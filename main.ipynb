{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# limit GPU growth\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(len(physical_devices))\n",
    "assert len(physical_devices) > 0, 'Not enough GPU hardware devices available'\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from run_step import *\n",
    "from datasets import *\n",
    "from containers import *\n",
    "from losses import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_train.txt\n",
      "\tNumber of classes: 119\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7fb76b1bfd08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7fb76b1bfd08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_train.txt\n",
      "\tNumber of classes: 119\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7fb924d94598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7fb924d94598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_test.txt\n",
      "\tNumber of classes: 30\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7fb76009b9d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7fb76009b9d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "Data Loader\n",
      "\tRoot: ../FUNIT/datasets/animals/\n",
      "\tList: ../FUNIT/datasets/animals_list_test.txt\n",
      "\tNumber of classes: 30\n",
      "WARNING:tensorflow:Entity <function get_tf_dataset.<locals>.<lambda> at 0x7fb76009b7b8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function get_tf_dataset.<locals>.<lambda> at 0x7fb76009b7b8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "epoch 0: \n",
      "INFO:tensorflow:batch_all_reduce: 48 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 50 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 48 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 50 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      " G_loss: 102.1634, D_loss: 0.0925\r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--config',\n",
    "#                         type=str,\n",
    "#                         default='../FUNIT/configs/funit_animals.yaml',\n",
    "#                         help='configuration file for training and testing')\n",
    "#     parser.add_argument('--batch_size', type=int, default=0)\n",
    "#     parser.add_argument('--output_path',\n",
    "#                         type=str,\n",
    "#                         default='.',\n",
    "#                         help=\"outputs path\")\n",
    "#     parser.add_argument('--test_batch_size',\n",
    "#                          type=int,\n",
    "#                          default=4)\n",
    "#     opts = parser.parse_args()\n",
    "#     config = get_config(opts.config)\n",
    "    config = get_config('./configs/funit_animals.yaml')\n",
    "    \n",
    "#     if opts.batch_size != 0:\n",
    "#         config['batch_size'] = opts.batch_size\n",
    "\n",
    "    # Strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    # num_gpus_available = strategy.num_replicas_in_sync\n",
    "\n",
    "    # Datasets\n",
    "    datasets = get_datasets(config)\n",
    "    # - Train\n",
    "    train_content_dataset = datasets[0]\n",
    "    train_class_dataset = datasets[1]\n",
    "    train_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))\n",
    "    global_batch_size = config['batch_size']\n",
    "    def train_ds_fn(input_context):\n",
    "        batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n",
    "        d = train_dataset.batch(batch_size)\n",
    "        return d.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)\n",
    "    dist_train_dataset = strategy.experimental_distribute_datasets_from_function(train_ds_fn)\n",
    "    # - Test\n",
    "#     test_content_dataset = datasets[2]\n",
    "#     test_class_dataset = datasets[3]\n",
    "#     test_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))\n",
    "#     def test_ds_fn(input_context):\n",
    "#         batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n",
    "#         d = test_dataset.batch(batch_size)\n",
    "#         return d.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)\n",
    "#     dist_test_dataset = strategy.experimental_distribute_datasets_from_function(test_ds_fn)\n",
    "    \n",
    "    # Mean loss\n",
    "    lossnames = [\"G_loss\",\"D_loss\"]\n",
    "    metrics_list = []\n",
    "    for itemname in lossnames:\n",
    "        metrics_list.append(tf.keras.metrics.Mean(itemname, dtype=tf.float32))\n",
    "    \n",
    "    EPOCHS = config['max_iter']\n",
    "    # Networks\n",
    "    with strategy.scope():\n",
    "        networks = FUNIT(config)\n",
    "\n",
    "        def gen_train_step(x, config):\n",
    "            co_data, cl_data = x\n",
    "            xa, la = co_data\n",
    "            xb, lb = cl_data\n",
    "            global_batch_size = config['batch_size']\n",
    "            with tf.GradientTape() as g_tape:\n",
    "                xt_g, xr, xa_gan_feat, xb_gan_feat = networks.gen_update(co_data,cl_data,config)\n",
    "                \n",
    "                resp_xr_fake, xr_gan_feat = networks.dis(xr, la)\n",
    "                resp_xt_fake, xt_gan_feat = networks.dis(xt_g, lb)\n",
    "                \n",
    "                # Generator - GAN loss\n",
    "                l_adv_t = GANloss.gen_loss(resp_xt_fake,lb)\n",
    "                l_adv_r = GANloss.gen_loss(resp_xr_fake,la)\n",
    "                l_adv = 0.5 * (l_adv_t + l_adv_r)\n",
    "                # Generator - Reconstruction loss\n",
    "                l_x_rec = recon_loss(xr, xa)\n",
    "                l_x_rec = tf.reduce_sum(l_x_rec) / global_batch_size\n",
    "                # Generator - Feature Matching loss\n",
    "                l_c_rec = featmatch_loss(xr_gan_feat, xa_gan_feat)\n",
    "                l_c_rec = tf.reduce_sum(l_c_rec) / global_batch_size\n",
    "                l_m_rec = featmatch_loss(xt_gan_feat, xb_gan_feat)\n",
    "                l_m_rec = tf.reduce_sum(l_m_rec) / global_batch_size\n",
    "                \n",
    "                G_loss = config['gan_w'] * l_adv +\\\n",
    "                         config['r_w'] * l_x_rec +\\\n",
    "                         config['fm_w'] * (l_c_rec + l_m_rec)\n",
    "                \n",
    "                loss = G_loss * (1.0 / global_batch_size)\n",
    "            gen_grad = g_tape.gradient(loss, networks.gen.trainable_variables)\n",
    "            networks.opt_gen.apply_gradients(zip(gen_grad, networks.gen.trainable_variables))\n",
    "            return G_loss\n",
    "        \n",
    "        def dis_train_step(x, config):\n",
    "            co_data, cl_data = x\n",
    "            xa, la = co_data\n",
    "            xb, lb = cl_data\n",
    "            with tf.GradientTape() as d_tape:\n",
    "                resp_real, real_gen_feat, xt_d, resp_fake, fake_gan_feat =\\\n",
    "                                                    networks.dis_update(co_data,cl_data,config)\n",
    "                # Discriminator - GAN loss\n",
    "                l_real = GANloss.dis_loss(resp_real, lb, 'real')\n",
    "                l_fake = GANloss.dis_loss(resp_fake, lb, 'fake')\n",
    "                # Discriminator - Gradient Penalty\n",
    "                l_reg = gradient_penalty(networks.dis, xb, lb)\n",
    "\n",
    "                D_loss = config['gan_w'] * l_real +\\\n",
    "                         config['gan_w'] * l_fake +\\\n",
    "                         10 * l_reg\n",
    "                loss = D_loss * (1.0 / config['batch_size'])\n",
    "            dis_grad = d_tape.gradient(loss, networks.dis.trainable_variables)\n",
    "            networks.opt_dis.apply_gradients(zip(dis_grad, networks.dis.trainable_variables))\n",
    "            return D_loss\n",
    "    \n",
    "    # Problem - https://www.tensorflow.org/tutorials/distribute/custom_training\n",
    "    with strategy.scope():\n",
    "        \n",
    "        @tf.function\n",
    "        def distributed_train_step(dataset_inputs, config):\n",
    "            dis_per_replica_losses = strategy.experimental_run_v2(dis_train_step, args=(dataset_inputs, config))\n",
    "            dis_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, dis_per_replica_losses, axis=None)\n",
    "            \n",
    "            gen_per_replica_losses = strategy.experimental_run_v2(gen_train_step, args=(dataset_inputs, config))\n",
    "            gen_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, gen_per_replica_losses, axis=None)\n",
    "            return gen_loss, dis_loss\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            print(\"epoch %d: \" % epoch)\n",
    "            for x in dist_train_dataset:\n",
    "                G_loss, D_loss = distributed_train_step(x, config)\n",
    "                print(\" G_loss: %.4f, D_loss: %.4f\" % (G_loss,D_loss), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for epoch in range(epochs):\n",
    "        print(\"epoch %d:\" % epoch)\n",
    "        for (co_data, cl_data) in train_dataset:\n",
    "            dis_per_replica_result = strategy.experimental_run_v2(train_dis, args=(networks, co_data, cl_data, config))\n",
    "            dis_loss = strategy.reduce(tf.distribute.ReduceOp.SUM,\n",
    "                                        dis_per_replica_result,\n",
    "                                        axis=None)\n",
    "            gen_per_replica_result = strategy.experimental_run_v2(train_gen, args=(networks, co_data, cl_data, config))\n",
    "            gen_loss = strategy.reduce(tf.distribute.ReduceOp.SUM,\n",
    "                                        gen_per_replica_result,\n",
    "                                        axis=None)\n",
    "            print(\" G_loss: %.4f, D_loss: %.4f\" % (gen_loss,dis_loss), end='\\r')\n",
    "\n",
    "#             train_returns = train_step(networks,co_data,cl_data,config)\n",
    "#             print(\" G_loss: %.4f, D_loss: %.4f\" % (train_returns['G_loss'],train_returns['D_loss']), end='\\r')\n",
    "#             for idx, itemname in enumerate(lossnames):\n",
    "#                 metrics_list[idx](train_returns[itemname])\n",
    "\n",
    "#         for idx, itemname in enumerate(lossnames):\n",
    "#             print(\"    {}: {:.4f}\".format(itemname,metrics_list[idx].result()))\n",
    "#             metrics_list[idx].reset_states()\n",
    "\n",
    "        '''if epoch % config['image_save_iter'] == 0 or\\\n",
    "            epoch % config['image_display_iter'] == 0:\n",
    "            if epoch % config['image_save_iter'] == 0:\n",
    "                key_str = '%08d' % (epoch + 1)\n",
    "            else:\n",
    "                key_str = 'current'\n",
    "            output_train_dataset = train_dataset.take(opts.test_batch_size)\n",
    "            output_test_dataset = test_dataset.take(opts.test_batch_size)\n",
    "            for idx, (co_data, cl_data) in output_train_dataset.enumerate():\n",
    "                test_returns = test_step(co_data,cl_data,config)\n",
    "                write_images(zip(test_returns['xa'],test_returns['xr'],test_returns['xt'],test_returns['xb']), \n",
    "                             test_returns['display_list'],\n",
    "                             'train_%s_%02d' % (key_str, idx),\n",
    "                             max(config['crop_image_height'], config['crop_image_width']))\n",
    "            for idx, (co_data, cl_data) in output_test_dataset.enumerate():\n",
    "                test_returns = test_step(co_data,cl_data,config)\n",
    "                write_images(zip(test_returns['xa'],test_returns['xr'],test_returns['xt'],test_returns['xb']), \n",
    "                             test_returns['display_list'],\n",
    "                             'test_%s_%02d' % (key_str, idx),\n",
    "                             max(config['crop_image_height'], config['crop_image_width']))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of working processes of output.\n",
    "#================================ TensorBoard (later)\n",
    "#================================ Check Points (later)\n",
    "#================================ Distributed Training (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = tf.random.normal([64,128,128,3])\n",
    "test2 = tf.random.normal([2,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block = ContentEncoder(3,2,64,'in','relu','reflect')\n",
    "# block = ClassEncoder(4,64,64,'none','relu','reflect')\n",
    "# block = Decoder(3,2,32,3,'relu','reflect')\n",
    "# block = MLP(32,256,3,'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
