{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from run_step import *\n",
    "from datasets import *\n",
    "from containers import *\n",
    "from losses import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--config',\n",
    "#                         type=str,\n",
    "#                         default='../FUNIT/configs/funit_animals.yaml',\n",
    "#                         help='configuration file for training and testing')\n",
    "#     parser.add_argument('--batch_size', type=int, default=0)\n",
    "#     parser.add_argument('--output_path',\n",
    "#                         type=str,\n",
    "#                         default='./outputs',\n",
    "#                         help=\"outputs path\")\n",
    "#     parser.add_argument('--multigpus',\n",
    "#                         action=\"store_true\")\n",
    "#     parser.add_argument('--test_batch_size',\n",
    "#                          type=int,\n",
    "#                          default=4)\n",
    "#     opts = parser.parse_args()\n",
    "#     config = get_config(opts.config)\n",
    "    config = get_config('./configs/funit_animals.yaml')\n",
    "    output_dir = \"./outputs/\"\n",
    "    \n",
    "#     if opts.batch_size != 0:\n",
    "#         config['batch_size'] = opts.batch_size\n",
    "\n",
    "    # Strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    # Datasets\n",
    "    datasets = get_datasets(config)\n",
    "    # - Train\n",
    "    train_content_dataset = datasets[0]\n",
    "    train_class_dataset = datasets[1]\n",
    "    train_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))\n",
    "    global_batch_size = config['batch_size']\n",
    "    def train_ds_fn(input_context):\n",
    "        batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n",
    "        d = train_dataset.batch(batch_size)\n",
    "        return d.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)\n",
    "    dist_train_dataset = strategy.experimental_distribute_datasets_from_function(train_ds_fn)\n",
    "    # - Test\n",
    "    test_content_dataset = datasets[2]\n",
    "    test_class_dataset = datasets[3]\n",
    "    test_dataset = tf.data.Dataset.zip((train_content_dataset, train_class_dataset))\n",
    "    \n",
    "    EPOCHS = config['max_iter']\n",
    "    # Networks\n",
    "    with strategy.scope():\n",
    "        networks = FUNIT(config)\n",
    "        test_networks = FUNIT(config)\n",
    "        \n",
    "        #有 networks 在裡頭，不知道如何分離\n",
    "        def gen_train_step(x, config):\n",
    "            co_data, cl_data = x\n",
    "            xa, la = co_data\n",
    "            xb, lb = cl_data\n",
    "            with tf.GradientTape() as g_tape:\n",
    "                xt_g, xr, xa_gan_feat, xb_gan_feat = networks.gen_update(co_data,cl_data,config)\n",
    "                \n",
    "                resp_xr_fake, xr_gan_feat = networks.dis(xr, la)\n",
    "                resp_xt_fake, xt_gan_feat = networks.dis(xt_g, lb)\n",
    "                \n",
    "                # Generator - GAN loss\n",
    "                l_adv_t = GANloss.gen_loss(resp_xt_fake,lb)\n",
    "                l_adv_r = GANloss.gen_loss(resp_xr_fake,la)\n",
    "                # - NOTICE\n",
    "                l_adv = 0.5 * (l_adv_t + l_adv_r)\n",
    "                # Generator - Reconstruction loss\n",
    "                l_x_rec = recon_loss(xr, xa)\n",
    "                l_x_rec = tf.reduce_mean(l_x_rec)\n",
    "                # Generator - Feature Matching loss\n",
    "                l_c_rec = featmatch_loss(xr_gan_feat, xa_gan_feat)\n",
    "                l_c_rec = tf.reduce_mean(l_c_rec)\n",
    "                l_m_rec = featmatch_loss(xt_gan_feat, xb_gan_feat)\n",
    "                l_m_rec = tf.reduce_mean(l_m_rec)\n",
    "                \n",
    "                G_loss = config['gan_w'] * l_adv +\\\n",
    "                         config['r_w'] * l_x_rec +\\\n",
    "                         config['fm_w'] * (l_c_rec + l_m_rec)\n",
    "                \n",
    "                loss = G_loss * (1.0 / config['batch_size'])\n",
    "            gen_grad = g_tape.gradient(loss, networks.gen.trainable_variables)\n",
    "            networks.opt_gen.apply_gradients(zip(gen_grad, networks.gen.trainable_variables))\n",
    "            return G_loss\n",
    "        \n",
    "        def dis_train_step(x, config):\n",
    "            co_data, cl_data = x\n",
    "            xa, la = co_data\n",
    "            xb, lb = cl_data\n",
    "            with tf.GradientTape() as d_tape:\n",
    "                resp_real, real_gen_feat, xt_d, resp_fake, fake_gan_feat =\\\n",
    "                                                    networks.dis_update(co_data,cl_data,config)\n",
    "                # Discriminator - GAN loss\n",
    "                l_real = GANloss.dis_loss(resp_real, lb, 'real')\n",
    "                l_fake = GANloss.dis_loss(resp_fake, lb, 'fake')\n",
    "                # Discriminator - Gradient Penalty\n",
    "                l_reg = gradient_penalty(networks.dis, xb, lb)\n",
    "\n",
    "                D_loss = config['gan_w'] * l_real +\\\n",
    "                         config['gan_w'] * l_fake +\\\n",
    "                         10 * l_reg\n",
    "                loss = D_loss * (1.0 / config['batch_size'])\n",
    "            dis_grad = d_tape.gradient(loss, networks.dis.trainable_variables)\n",
    "            networks.opt_dis.apply_gradients(zip(dis_grad, networks.dis.trainable_variables))\n",
    "            return D_loss\n",
    "            \n",
    "    \n",
    "    # Problem - https://www.tensorflow.org/tutorials/distribute/custom_training\n",
    "    with strategy.scope():\n",
    "        \n",
    "        @tf.function\n",
    "        def distributed_train_step(dataset_inputs, config):\n",
    "            dis_per_replica_losses = strategy.experimental_run_v2(dis_train_step, args=(dataset_inputs, config))\n",
    "            dis_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, dis_per_replica_losses, axis=None)\n",
    "            \n",
    "            gen_per_replica_losses = strategy.experimental_run_v2(gen_train_step, args=(dataset_inputs, config))\n",
    "            gen_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, gen_per_replica_losses, axis=None)\n",
    "            return gen_loss, dis_loss\n",
    "        \n",
    "        \n",
    "        # Checkpoint\n",
    "        checkpoint_dir = './training_checkpoints'\n",
    "        gen_ckpt_prefix = os.path.join(checkpoint_dir, \"gen_ckpt\")\n",
    "        dis_ckpt_prefix = os.path.join(checkpoint_dir, \"dis_ckpt\")\n",
    "        gen_ckpt = tf.train.Checkpoint(optimizer= networks.opt_gen, net= networks.gen)\n",
    "        dis_ckpt = tf.train.Checkpoint(optimizer= networks.opt_dis, net= networks.dis)\n",
    "        test_gen_ckpt = tf.train.Checkpoint(optimizer= test_networks.opt_gen, net= test_networks.gen)\n",
    "        \n",
    "        iteration = 1\n",
    "        for epoch in range(1,EPOCHS+1):\n",
    "            print(\"epoch %d: \" % epoch)\n",
    "                \n",
    "            for x in dist_train_dataset:\n",
    "                start_time = time.time()\n",
    "                G_loss, D_loss = distributed_train_step(x, config)\n",
    "                print(\" (%d/%d) G_loss: %.4f, D_loss: %.4f, time: %.5f\" % (iteration,config['max_iter'],G_loss,D_loss,(time.time() - start_time)))\n",
    "            \n",
    "                # Test Step (Print this interval result)\n",
    "                if iteration % config['image_save_iter'] == 0 or\\\n",
    "                   iteration % config['image_display_iter'] == 0:\n",
    "                    gen_ckpt.save(os.path.join(gen_ckpt_prefix, \"ckpt\"))\n",
    "                    dis_ckpt.save(os.path.join(dis_ckpt_prefix, \"ckpt\"))\n",
    "                    print(\"load newest ckpt file: %s\" % tf.train.latest_checkpoint(gen_ckpt_prefix))\n",
    "                    test_gen_ckpt.restore(tf.train.latest_checkpoint(gen_ckpt_prefix))\n",
    "                    if iteration % config['image_save_iter'] == 0:\n",
    "                        key_str = '%08d' % iteration\n",
    "                    else:\n",
    "                        key_str = 'current'\n",
    "                    output_train_dataset = train_dataset.batch(global_batch_size).take(4) # opts.test_batch_size\n",
    "                    output_test_dataset = test_dataset.batch(global_batch_size).take(4) # opts.test_batch_size\n",
    "                    for idx,(co_data, cl_data) in output_train_dataset.enumerate():\n",
    "                        test_returns = test_step(test_networks,co_data,cl_data,config)\n",
    "                        write_images((test_returns['xa'],test_returns['xr'],test_returns['xt'],test_returns['xb']), \n",
    "                                     test_returns['display_list'],\n",
    "                                     os.path.join(output_dir, 'train_%s_%02d' % (key_str, idx)),\n",
    "                                     max(config['crop_image_height'], config['crop_image_width']))\n",
    "                    for idx,(co_data, cl_data) in output_test_dataset.enumerate():\n",
    "                        test_returns = test_step(test_networks,co_data,cl_data,config)\n",
    "                        write_images((test_returns['xa'],test_returns['xr'],test_returns['xt'],test_returns['xb']), \n",
    "                                     test_returns['display_list'],\n",
    "                                     os.path.join(output_dir, 'test_%s_%02d' % (key_str, idx)),\n",
    "                                     max(config['crop_image_height'], config['crop_image_width']))\n",
    "                    \n",
    "                iteration += 1\n",
    "                if iteration >= config['max_iter']:\n",
    "                    print(\"End of iteration\")\n",
    "                    break\n",
    "            if iteration >= config['max_iter']:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of working processes of output.\n",
    "# Split Distributed training example: \n",
    "# -- https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/nmt_with_attention/distributed_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = tf.random.normal([64,128,128,3])\n",
    "test2 = tf.random.normal([2,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block = ContentEncoder(3,2,64,'in','relu','reflect')\n",
    "# block = ClassEncoder(4,64,64,'none','relu','reflect')\n",
    "# block = Decoder(3,2,32,3,'relu','reflect')\n",
    "# block = MLP(32,256,3,'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
